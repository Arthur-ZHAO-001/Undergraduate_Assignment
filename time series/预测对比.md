# 机器学习模型与Space State模型预测效果对比

## 18 数科 赵呈亮 201800820179



​	**本文档介绍了机器学习算法与Space State模型在30列时间序列上预测效果的对比。其中机器学习部分选取 随机森林 与长短期记忆（LSTM）。Space State部分选取了课上使用的五种预测方法。预测评价标准选取平均绝对百分比误差（MAPE）。对比在30列数据上的每一列 分别训练模型进行预测。 预测分为两种，短期预测中，随机森林在30列数据上表现最优。长期预测中，各个模型的表现均不如短期预测。单随机森林算法依然表现最优**



说明：

文档内图片采取网络URL嵌入

## 1.数据

    所用数据为美股 2019-05-06至2020-07-04 阶段各大公司的股票收盘价格。 共共301行，102列 表示共有301个交易日，101家公司。每一数据点代表此公司在当前交易日的收盘价格。每一列均为时间序列数据.


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# import warnings filter

df=pd.read_csv("rdata.csv")
df=df.drop(df.columns[0],axis=1)
column=df.columns

```


```python
df.head()
```

**以下是数据前五行，列名为公司股票代码**


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL.Close</th>
      <th>ABBV.Close</th>
      <th>ABT.Close</th>
      <th>ACN.Close</th>
      <th>ADBE.Close</th>
      <th>AIG.Close</th>
      <th>ALL.Close</th>
      <th>AMGN.Close</th>
      <th>AMT.Close</th>
      <th>AMZN.Close</th>
      <th>...</th>
      <th>UNH.Close</th>
      <th>UNP.Close</th>
      <th>UPS.Close</th>
      <th>USB.Close</th>
      <th>V.Close</th>
      <th>VZ.Close</th>
      <th>WBA.Close</th>
      <th>WFC.Close</th>
      <th>WMT.Close</th>
      <th>XOM.Close</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>208.479996</td>
      <td>79.260002</td>
      <td>79.070000</td>
      <td>176.259995</td>
      <td>283.660004</td>
      <td>47.110001</td>
      <td>97.680000</td>
      <td>177.059998</td>
      <td>192.179993</td>
      <td>1950.550049</td>
      <td>...</td>
      <td>240.500000</td>
      <td>177.160004</td>
      <td>104.620003</td>
      <td>53.340000</td>
      <td>162.279999</td>
      <td>56.910000</td>
      <td>53.919998</td>
      <td>48.430000</td>
      <td>102.459999</td>
      <td>77.129997</td>
    </tr>
    <tr>
      <td>1</td>
      <td>202.860001</td>
      <td>77.949997</td>
      <td>76.910004</td>
      <td>173.940002</td>
      <td>277.070007</td>
      <td>50.299999</td>
      <td>95.660004</td>
      <td>173.869995</td>
      <td>189.850006</td>
      <td>1921.000000</td>
      <td>...</td>
      <td>238.050003</td>
      <td>174.750000</td>
      <td>102.089996</td>
      <td>52.560001</td>
      <td>160.210007</td>
      <td>56.630001</td>
      <td>52.939999</td>
      <td>47.169998</td>
      <td>101.300003</td>
      <td>76.720001</td>
    </tr>
    <tr>
      <td>2</td>
      <td>202.899994</td>
      <td>77.989998</td>
      <td>76.220001</td>
      <td>173.820007</td>
      <td>276.769989</td>
      <td>49.959999</td>
      <td>95.320000</td>
      <td>172.149994</td>
      <td>191.639999</td>
      <td>1917.770020</td>
      <td>...</td>
      <td>239.149994</td>
      <td>175.580002</td>
      <td>101.849998</td>
      <td>52.220001</td>
      <td>160.759995</td>
      <td>56.380001</td>
      <td>53.500000</td>
      <td>47.000000</td>
      <td>100.300003</td>
      <td>76.839996</td>
    </tr>
    <tr>
      <td>3</td>
      <td>200.720001</td>
      <td>77.910004</td>
      <td>76.419998</td>
      <td>174.460007</td>
      <td>275.790009</td>
      <td>50.830002</td>
      <td>94.389999</td>
      <td>172.869995</td>
      <td>192.520004</td>
      <td>1899.869995</td>
      <td>...</td>
      <td>237.970001</td>
      <td>175.630005</td>
      <td>100.750000</td>
      <td>52.119999</td>
      <td>159.809998</td>
      <td>56.480000</td>
      <td>53.560001</td>
      <td>46.740002</td>
      <td>99.540001</td>
      <td>76.769997</td>
    </tr>
    <tr>
      <td>4</td>
      <td>197.179993</td>
      <td>77.449997</td>
      <td>76.449997</td>
      <td>174.300003</td>
      <td>278.480011</td>
      <td>51.639999</td>
      <td>96.379997</td>
      <td>171.850006</td>
      <td>195.660004</td>
      <td>1889.979980</td>
      <td>...</td>
      <td>240.589996</td>
      <td>175.919998</td>
      <td>100.629997</td>
      <td>52.169998</td>
      <td>160.710007</td>
      <td>56.910000</td>
      <td>53.419998</td>
      <td>47.150002</td>
      <td>101.910004</td>
      <td>76.559998</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 101 columns</p>
</div>



以苹果公司（AAPL）的股票价格为例，其在301个交易日的股价走势如下图


```python
plt.plot(df["AAPL.Close"])
```

<img src="https://s1.ax1x.com/2020/08/15/dk6GmF.png" alt="output 5 1" border="0">


## 2. 预测评价标准

此处预测标准我们选取MAPE，平均绝对百分比误差（Mean Absolute Percentage Error）作为预测方法的评价标准。其计算方法为$$ MAPE=\frac{100%}{n}\sum_{i=1}^{n}\frac{\left | \hat{y_i}-y_i \right |}{\left | y_i \right |}$$

MAPE的值越小，说明模型效果预测越好
因为所用数据绝对值较大，不需要使用对称平均绝对百分比误差（SMAPE）作为评价标准。


```python
def mape(y_true, y_pred):
    return np.mean(np.abs((y_pred - y_true) / y_true)) * 100
#!!! 此处返回的是百分数的形式
```

## 3. 预测方法

### 机器学习
* **随机森林（RandomForest**
* **长短期记忆（LSTM）**
### Space State 模型
* **使用下列五种方法**

### 1.多误差来源AR模型(MSOE) 
#### 使用卡尔曼滤波（Kalman filter）预测


```python
def ForecastARkf(y,h):#y为待预测序列 h为预测步长 返回预测结果
    n=len(y)
    a=np.zeros(n)
    p=np.zeros(n)
    a[0]=y[0]
    p[0]=10000;
    k=np.zeros(n)
    v=np.zeros(n)
    z=1
    def funcTheta(X):
        w=1-exp(-abs(X[2]))
        q=abs(X[0])
        co=abs(X[1])
        likelihood=0
        sigmae=0
        for t in range(1,n):
        
            k[t]=(z*w*p[t-1])/(z**2*p[t-1]+1)#卡尔曼滤波部分
            p[t]=w**2*p[t-1]-w*z*k[t]*p[t-1]+q
            v[t]=y[t]-z*a[t-1]
            a[t]=co+w*a[t-1]+k[t]*v[t]
            sigmae=sigmae+(v[t]**2/(z**2*p[t-1]+1))
            likelihood=likelihood+.5*np.log(2*pi)+.5+.5*np.log(z**2*p[t-1]+1)

        likelihood=likelihood+.5*n*np.log(sigmae/n) 
        return likelihood
    res = minimize(funcTheta, [0.5,10,2])
    q=abs(res.x[0])
    co=abs(res.x[1]);
    w=1-exp(-res.x[2])
    sigmae=0
    sigmae<-0    
    for t in range(1,n):
        k[t]=(z*w*p[t-1])/(z**2*p[t-1]+1)
        p[t]=w**2*p[t-1]-w*z*k[t]*p[t-1]+q
        v[t]=y[t]-z*a[t-1]
        a[t]=co+w*a[t-1]+k[t]*v[t]
        sigmae=sigmae+(v[t]**2/(z**2*p[t-1]+1))
  
    
    
    Forec=np.zeros(h)#预测部分
    Forec[0]=a[len(y)-1]
    for i in range(1,h):
        Forec[i]=co+w*Forec[i-1];
    
    return Forec 
```

### 2.单误差来源AR模型（SSOE）


```python
def ForecastAR(y,h):#y为待预测序列 h为预测步长 返回预测结果
  
    state=np.zeros(len(y))
    v=np.zeros(len(y))
    state[0]=y[0]
    def logLikConc(myparam):
        w= 1-exp(-abs(myparam[0]))
        gamma=abs(myparam[1])
        co=abs(myparam[2])
        for t in range(1,len(y)): 
            v[t]=y[t]-state[t-1]
            state[t] =co+w*state[t-1]+gamma*v[t]
  
        return np.sum(v[:len(y)]**2)
    res = minimize(logLikConc, [2,.2,1])
    w=1-exp(-abs(res.x[0]))
    gamma=abs(res.x[1])
    co=abs(res.x[2])
  
    for t in range(1,len(y)): 
        v[t]=y[t]-state[t-1]
        state[t] =co+w*state[t-1]+gamma*v[t]
  
  
    Forec=np.zeros(h)
    Forec[0]=state[len(y)-1]
    for i in range(1,h):
        Forec[i]=co+w*Forec[i-1];
  
    return Forec


```

### 3. 多误差来源ThetaMethod模型
#### 使用卡尔曼滤波（Kalman filter）预测


```python
def ForecastThetakf(y,h):#y为待预测序列 h为预测步长 返回预测结果
    
    n=len(y)
    a=np.zeros(n)
    p=np.zeros(n)
    a[0]=y[0]
    p[0]=10000;
    k=np.zeros(n)
    v=np.zeros(n)

    def funcTheta(parameters):
        q=abs(parameters[0])
        co=abs(parameters[1]);
        z=1
        w=1
        likelihood=0
        sigmae=0
        for t in range(1,n):
        
            k[t]=(z*w*p[t-1])/(z**2*p[t-1]+1)#卡尔曼滤波部分
            p[t]=w**2*p[t-1]-w*z*k[t]*p[t-1]+q
            v[t]=y[t]-z*a[t-1]
            a[t]=co+w*a[t-1]+k[t]*v[t]
            sigmae=sigmae+(v[t]**2/(z**2*p[t-1]+1))
            likelihood=likelihood+.5*np.log(2*pi)+.5+.5*np.log(z**2*p[t-1]+1)

        likelihood=likelihood+.5*n*np.log(sigmae/n) 
        return likelihood
    result=minimize(funcTheta, [.3,1])
    w=1
    z=1
    q=abs(result.x[0])
    co=abs(result.x[1]); 
  
  
    for t in range(1,len(y)):
        k[t]=(z*w*p[t-1])/(z**2*p[t-1]+1)
        p[t]=w**2*p[t-1]-w*z*k[t]*p[t-1]+q
        v[t]=y[t]-z*a[t-1]
        a[t]=co+w*a[t-1]+k[t]*v[t]
  
  
    Forecast=np.zeros(h)
    Forecast[0]=a[-1]
    for i in range(1,h):
        Forecast[i]=co+Forecast[i-1];
  
    return Forecast

```

### 4. 单误差来源ThetaMethod模型


```python
def ForecastTheta(y,h):#y为待预测序列 h为预测步长 返回预测结果
  
    state=np.zeros(len(y))
    v=np.zeros(len(y))
    state[0]=y[0]
    def logLikConc(myparam):
        w=1
        gamma=abs(myparam[0])
        co=abs(myparam[1])
        for t in range(1,len(y)):
            v[t]=y[t]-state[t-1]
            state[t] =co+w*state[t-1]+gamma*v[t]
  
        return np.sum(v[:len(y)]**2)
    result=minimize(logLikConc, [.3,1])
    w=1
    gamma=abs(result.x[0])
    co=abs(result.x[1]); 
  
    for t in range(1,len(y)):
        v[t]=y[t]-state[t-1]
        state[t] =co+w*state[t-1]+gamma*v[t]
  
  
    Forec=np.zeros(h)
    Forec[0]=state[len(y)-1]
    for i in range(1,h):
        Forec[i]=co+w*Forec[i-1];
  
    return Forec

```

### 5.单误差阻滞模型


```python
def ForecastDamped(y,h):#y为待预测序列 h为预测步长 返回预测结果
  
    obs=len(y)
    damped=np.zeros((obs,2))
    damped[0][0]=y[1]
    damped[0][1]=0
  
    inn=np.zeros(obs)
  
  
    def fmsoe(param):
        k1=abs(param[0])
        k2=abs(param[1])
        k3=abs(param[2])
  
        for t in range(1,obs) :
            inn[t]=y[t]-damped[t-1][0]-k3*damped[t-1][1]
            damped[t][0] = damped[t-1][0]+k3*damped[t-1][1]+k1*inn[t]
            damped[t][1] = k3*damped[t-1][1]+k2*inn[t]
    
  
        return np.sum(inn[:obs]**2)/obs
  
    result=minimize(fmsoe,np.random.rand(3))
  
    k1=abs(result.x[0])
    k2=abs(result.x[1])
    k3=abs(result.x[2])
    if k3>1:
        k3=1
  
    for  t in range(1,obs):
    
        inn[t]=y[t]-damped[t-1][0]-k3*damped[t-1][1]
        damped[t][0] = damped[t-1][0]+k3*damped[t-1][1]+k1*inn[t]
        damped[t][1] = k3*damped[t-1][1]+k2*inn[t]
    
  
  
    Forecast=np.zeros(h)
    Forecast[0]=damped[obs-1][0]+k3*damped[obs-1][1]
    for i in range(1,h):
        Forecast[i]=Forecast[i-1]+damped[obs-1][1]*k3**i;
  
    return Forecast

```

# 预测过程

**在预测中，我们选取数据表中前30列数据。用不同方法分别对这30列数据进行预测并计算其MAPE值。 最后得到30列数据中，每一列数据的最优预测算法，与30列数据的平均最优算法**

### 短期预测（预测步长=6）
#### 1.随机森林

**随机森林算法既可处理分类问题又可处理回归问题，在这里我们选取随机森林算法来处理回归问题**

* **数据预处理**
因为模型不能直接处理时间序列数据，所以要先将数据转化为可处理的形式。 选取当前股价作为目标值，输入特征为平移特征，分别为其前n个值。以苹果公司（AAPL）为例


```python
data=pd.DataFrame(df["AAPL.Close"].copy())
col=data.columns[0]
for i in range(1,16):
    lag=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
    for j in range(15,len(data)):
        lag.append(data[col][j-i])
    data["lag"+str(i)]=lag
```


```python
new_data=data[15:].copy()
train=new_data[:-6]
test=new_data[-6:]
X_train=train.drop(train.columns[0],axis=1)
Y_train=train[train.columns[0]]
X_train=np.array(X_train)
Y_train=np.array(Y_train)
X_test=test.drop(test.columns[0],axis=1)
X_test=np.array(X_test)
Y_test=test[test.columns[0]]
Y_test=np.array(Y_test)
train
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL.Close</th>
      <th>lag1</th>
      <th>lag2</th>
      <th>lag3</th>
      <th>lag4</th>
      <th>lag5</th>
      <th>lag6</th>
      <th>lag7</th>
      <th>lag8</th>
      <th>lag9</th>
      <th>lag10</th>
      <th>lag11</th>
      <th>lag12</th>
      <th>lag13</th>
      <th>lag14</th>
      <th>lag15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>15</td>
      <td>178.229996</td>
      <td>178.970001</td>
      <td>179.660004</td>
      <td>182.779999</td>
      <td>186.600006</td>
      <td>183.089996</td>
      <td>189.000000</td>
      <td>190.080002</td>
      <td>190.919998</td>
      <td>188.660004</td>
      <td>185.720001</td>
      <td>197.179993</td>
      <td>200.720001</td>
      <td>202.899994</td>
      <td>202.860001</td>
      <td>208.479996</td>
    </tr>
    <tr>
      <td>16</td>
      <td>177.380005</td>
      <td>178.229996</td>
      <td>178.970001</td>
      <td>179.660004</td>
      <td>182.779999</td>
      <td>186.600006</td>
      <td>183.089996</td>
      <td>189.000000</td>
      <td>190.080002</td>
      <td>190.919998</td>
      <td>188.660004</td>
      <td>185.720001</td>
      <td>197.179993</td>
      <td>200.720001</td>
      <td>202.899994</td>
      <td>202.860001</td>
    </tr>
    <tr>
      <td>17</td>
      <td>178.300003</td>
      <td>177.380005</td>
      <td>178.229996</td>
      <td>178.970001</td>
      <td>179.660004</td>
      <td>182.779999</td>
      <td>186.600006</td>
      <td>183.089996</td>
      <td>189.000000</td>
      <td>190.080002</td>
      <td>190.919998</td>
      <td>188.660004</td>
      <td>185.720001</td>
      <td>197.179993</td>
      <td>200.720001</td>
      <td>202.899994</td>
    </tr>
    <tr>
      <td>18</td>
      <td>175.070007</td>
      <td>178.300003</td>
      <td>177.380005</td>
      <td>178.229996</td>
      <td>178.970001</td>
      <td>179.660004</td>
      <td>182.779999</td>
      <td>186.600006</td>
      <td>183.089996</td>
      <td>189.000000</td>
      <td>190.080002</td>
      <td>190.919998</td>
      <td>188.660004</td>
      <td>185.720001</td>
      <td>197.179993</td>
      <td>200.720001</td>
    </tr>
    <tr>
      <td>19</td>
      <td>173.300003</td>
      <td>175.070007</td>
      <td>178.300003</td>
      <td>177.380005</td>
      <td>178.229996</td>
      <td>178.970001</td>
      <td>179.660004</td>
      <td>182.779999</td>
      <td>186.600006</td>
      <td>183.089996</td>
      <td>189.000000</td>
      <td>190.080002</td>
      <td>190.919998</td>
      <td>188.660004</td>
      <td>185.720001</td>
      <td>197.179993</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>290</td>
      <td>361.779999</td>
      <td>353.630005</td>
      <td>364.839996</td>
      <td>360.059998</td>
      <td>366.529999</td>
      <td>358.869995</td>
      <td>349.720001</td>
      <td>351.730011</td>
      <td>351.589996</td>
      <td>352.079987</td>
      <td>342.989990</td>
      <td>338.799988</td>
      <td>335.899994</td>
      <td>352.839996</td>
      <td>343.989990</td>
      <td>333.459991</td>
    </tr>
    <tr>
      <td>291</td>
      <td>364.799988</td>
      <td>361.779999</td>
      <td>353.630005</td>
      <td>364.839996</td>
      <td>360.059998</td>
      <td>366.529999</td>
      <td>358.869995</td>
      <td>349.720001</td>
      <td>351.730011</td>
      <td>351.589996</td>
      <td>352.079987</td>
      <td>342.989990</td>
      <td>338.799988</td>
      <td>335.899994</td>
      <td>352.839996</td>
      <td>343.989990</td>
    </tr>
    <tr>
      <td>292</td>
      <td>364.109985</td>
      <td>364.799988</td>
      <td>361.779999</td>
      <td>353.630005</td>
      <td>364.839996</td>
      <td>360.059998</td>
      <td>366.529999</td>
      <td>358.869995</td>
      <td>349.720001</td>
      <td>351.730011</td>
      <td>351.589996</td>
      <td>352.079987</td>
      <td>342.989990</td>
      <td>338.799988</td>
      <td>335.899994</td>
      <td>352.839996</td>
    </tr>
    <tr>
      <td>293</td>
      <td>364.109985</td>
      <td>364.109985</td>
      <td>364.799988</td>
      <td>361.779999</td>
      <td>353.630005</td>
      <td>364.839996</td>
      <td>360.059998</td>
      <td>366.529999</td>
      <td>358.869995</td>
      <td>349.720001</td>
      <td>351.730011</td>
      <td>351.589996</td>
      <td>352.079987</td>
      <td>342.989990</td>
      <td>338.799988</td>
      <td>335.899994</td>
    </tr>
    <tr>
      <td>294</td>
      <td>373.850006</td>
      <td>364.109985</td>
      <td>364.109985</td>
      <td>364.799988</td>
      <td>361.779999</td>
      <td>353.630005</td>
      <td>364.839996</td>
      <td>360.059998</td>
      <td>366.529999</td>
      <td>358.869995</td>
      <td>349.720001</td>
      <td>351.730011</td>
      <td>351.589996</td>
      <td>352.079987</td>
      <td>342.989990</td>
      <td>338.799988</td>
    </tr>
  </tbody>
</table>
<p>280 rows × 16 columns</p>
</div>



**以第一列数据为例。如结果所示，第一列为股价数据，作为输出值。 输入特征为当前股价的前1-15天的数据。 滞后值如果过大，那么训练数据将过少。如滞后值过小，则无法描述趋势变化。所以经过测试，选取滞后值为15，**

**下面对30组数据进行预测**

**每一列分别进行数据处理，模型训练，预测等过程**


```python
from sklearn import ensemble
def RF_pred(h):
    MAPE_RF=[]
    pred_RF=[]
    for d in range(30):
        data=pd.DataFrame(df[column[d]].copy())
        col=data.columns[0]
        for i in range(1,16):
            lag=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
            for j in range(15,len(data)):
                lag.append(data[col][j-i])
            data["lag"+str(i)]=lag
        new_data=data[15:].copy()
        train=new_data[:-h]
        test=new_data[-h:]
        X_train=train.drop(train.columns[0],axis=1)
        Y_train=train[train.columns[0]]
        X_train=np.array(X_train)
        Y_train=np.array(Y_train)
        X_test=test.drop(test.columns[0],axis=1)
        X_test=np.array(X_test)
        Y_test=test[test.columns[0]]
        Y_test=np.array(Y_test)
        regr=ensemble.RandomForestRegressor()
        regr.fit(X_train,Y_train)
        Y_pred=regr.predict(X_test)
        MAPE_RF.append(mape(Y_test,Y_pred))
        pred_RF.append(Y_pred)
    print("30列随机森林的MAPE值为")    
    return MAPE_RF,pred_RF 
```


```python
MAPE_RF,pred_RF = RF_pred(6)
```


```python
plt.plot(df["ABBV.Close"])
plt.plot(range(295,301),pred_RF[1])
```

**以第二列（ABBV公司）数据为例，随机森林短期预测结果如下图**



<img src="https://s1.ax1x.com/2020/08/15/dk6Nk9.png" alt="output 27 1" border="0">


### 长短期记忆（LSTM）
**数据预处理方式与随机森林相同**


LSTM广泛用于序列预测问题，并且已被证明是非常有效的。它们工作得很好是因为LSTM能够存储过去重要的信息，并忘记不重要的信息。LSTM有三个门：

输入门：输入门将信息添加到单元状态
遗忘门：它删除模型不再需要的信息
输出门：LSTM的输出门选择要显示为输出的信息



```python
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

```

    Using TensorFlow backend.

```python
def LSTM_pred(h):#h为预测步长 
    MAPE_LSTM=[]
    pred_LSTM=[]
    for d in range(30):
        dataset=df[column[d]].values
        dataset=dataset.reshape(-1,1)
        train = dataset[:-h].copy()
        valid = dataset[-h:].copy()
        Y_test=np.array(valid)#拆分数据集
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_data = scaler.fit_transform(dataset)
        X_train, y_train = [], []
        test=scaled_data[-h:]
# 按每15天作为特征输入
        for i in range(15,len(train)):
            X_train.append(scaled_data[i-15:i, 0])
            y_train.append(scaled_data[i, 0])    
        X_train, y_train = np.array(X_train), np.array(y_train)
        X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1], 1))
        model = Sequential()#模型定义
        model.add(LSTM(units=30, return_sequences=True, input_shape=(X_train.shape[1],1)))
        model.add(LSTM(units=30))
        model.add(Dense(1))
        model.compile(loss='mean_squared_error', optimizer='adam')
        model.fit(X_train, y_train, epochs=1, batch_size=1, verbose=2)#模型训练
        inputs = dataset[len(dataset) - len(valid) - 15:]
        inputs = inputs.reshape(-1,1)
        inputs  = scaler.transform(inputs)
        inputs.shape
        X_test = []
        for i in range(15,inputs.shape[0]):
            X_test.append(inputs[i-15:i,0])#拆分测试集
        X_test = np.array(X_test)

        X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1], 1))
        Y_pred = model.predict(X_test)
        Y_pred= scaler.inverse_transform(Y_pred)#预测
        Y_pred = Y_pred.reshape((len(valid),))
        MAPE_LSTM.append(mape(Y_test,Y_pred))
        pred_LSTM.append(Y_pred)
    print("30列LSTM的MAPE值为")    
    return MAPE_LSTM,pred_LSTM    
    
    
```


```python
#进行预测
MAPE_LSTM,pred_LSTM=LSTM_pred(6)

```

## Space State模型


```python
from scipy.optimize import minimize_scalar
from scipy.optimize import minimize
from math import pi,exp
```


```python
def Space_State(h):#使用5种Space state模型进行预测
    MAPE_ARkf=[]
    MAPE_AR=[]
    MAPE_THkf=[]
    MAPE_TH=[]
    MAPE_Damp=[]

    for d in range(30):
        data=pd.DataFrame(df[column[d]].copy())
        data=np.array(data)
        y=data[:-h]
        y_real=data[-h:]
        MAPE_ARkf.append(mape(y_real,ForecastARkf(y,h)))
        MAPE_AR.append(mape(y_real,ForecastAR(y,h)))
        MAPE_THkf.append(mape(y_real,ForecastThetakf(y,h)))
        MAPE_TH.append(mape(y_real,ForecastTheta(y,h)))
        MAPE_Damp.append(mape(y_real,ForecastDamped(y,h)))
    
    
    
    return MAPE_ARkf,MAPE_AR,MAPE_THkf,MAPE_TH,MAPE_Damp
```


```python
MAPE_ARkf,MAPE_AR,MAPE_THkf,MAPE_TH,MAPE_Damp=Space_State(6)
```


```python
plt.plot(df["ABBV.Close"])
plt.plot(range(295,301),ForecastAR(df["ABBV.Close"][:-6],6))
```





**以第二列（ABBV公司）数据为例，随机森林短期预测结果如下图**



<img src="https://s1.ax1x.com/2020/08/15/dk6Jw4.png" alt="output 36 2" border="0">

**橙色为预测值**

## 预测结果


```python
MAPE=pd.DataFrame()
MAPE["RF"]=MAPE_RF
MAPE["LSTM"]=MAPE_LSTM
MAPE["ARkf"]=MAPE_ARkf
MAPE["AR"]=MAPE_AR
MAPE["Thetakf"]=MAPE_THkf
MAPE["Theta"]=MAPE_TH
MAPE["Damp"]=MAPE_Damp
MAPE
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
**这是不同模型分别在30列时间序列数据上的MAPE值**

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RF</th>
      <th>LSTM</th>
      <th>ARkf</th>
      <th>AR</th>
      <th>Thetakf</th>
      <th>Theta</th>
      <th>Damp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>3.645925</td>
      <td>1.511393</td>
      <td>2.247965</td>
      <td>2.266231</td>
      <td>2.247994</td>
      <td>2.266211</td>
      <td>1.585440</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.909376</td>
      <td>2.325148</td>
      <td>0.831578</td>
      <td>0.830701</td>
      <td>0.989250</td>
      <td>0.984250</td>
      <td>0.872200</td>
    </tr>
    <tr>
      <td>2</td>
      <td>2.012523</td>
      <td>6.238877</td>
      <td>1.411968</td>
      <td>1.398379</td>
      <td>0.646380</td>
      <td>0.648702</td>
      <td>0.788662</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.296676</td>
      <td>1.231033</td>
      <td>1.138844</td>
      <td>1.132251</td>
      <td>0.799348</td>
      <td>0.803657</td>
      <td>0.643889</td>
    </tr>
    <tr>
      <td>4</td>
      <td>2.833560</td>
      <td>2.444069</td>
      <td>2.187158</td>
      <td>2.187687</td>
      <td>2.187158</td>
      <td>2.187687</td>
      <td>2.346506</td>
    </tr>
    <tr>
      <td>5</td>
      <td>4.068560</td>
      <td>3.219746</td>
      <td>3.976707</td>
      <td>3.872365</td>
      <td>3.977330</td>
      <td>4.040563</td>
      <td>5.128283</td>
    </tr>
    <tr>
      <td>6</td>
      <td>2.446597</td>
      <td>6.750422</td>
      <td>8.745618</td>
      <td>8.676497</td>
      <td>7.769219</td>
      <td>7.743298</td>
      <td>8.382900</td>
    </tr>
    <tr>
      <td>7</td>
      <td>1.741997</td>
      <td>6.421634</td>
      <td>1.749137</td>
      <td>1.768653</td>
      <td>2.385556</td>
      <td>2.372551</td>
      <td>1.882151</td>
    </tr>
    <tr>
      <td>8</td>
      <td>1.215338</td>
      <td>10.633948</td>
      <td>1.385069</td>
      <td>1.371326</td>
      <td>1.554152</td>
      <td>1.541896</td>
      <td>1.495015</td>
    </tr>
    <tr>
      <td>9</td>
      <td>6.450180</td>
      <td>5.140846</td>
      <td>2.000363</td>
      <td>2.223760</td>
      <td>1.961436</td>
      <td>2.223760</td>
      <td>2.297750</td>
    </tr>
    <tr>
      <td>10</td>
      <td>3.587432</td>
      <td>3.800199</td>
      <td>4.731609</td>
      <td>3.571670</td>
      <td>3.661830</td>
      <td>3.574026</td>
      <td>5.678483</td>
    </tr>
    <tr>
      <td>11</td>
      <td>4.186880</td>
      <td>2.802759</td>
      <td>5.755354</td>
      <td>5.920712</td>
      <td>5.872094</td>
      <td>5.916286</td>
      <td>11.105363</td>
    </tr>
    <tr>
      <td>12</td>
      <td>1.815462</td>
      <td>5.403737</td>
      <td>2.488203</td>
      <td>2.479657</td>
      <td>2.468632</td>
      <td>2.467718</td>
      <td>2.498844</td>
    </tr>
    <tr>
      <td>13</td>
      <td>1.457908</td>
      <td>3.119747</td>
      <td>2.776682</td>
      <td>2.830216</td>
      <td>2.843905</td>
      <td>2.883542</td>
      <td>2.110885</td>
    </tr>
    <tr>
      <td>14</td>
      <td>2.147044</td>
      <td>3.770645</td>
      <td>1.478550</td>
      <td>1.475311</td>
      <td>1.472420</td>
      <td>1.471713</td>
      <td>1.480618</td>
    </tr>
    <tr>
      <td>15</td>
      <td>1.000089</td>
      <td>2.954294</td>
      <td>1.457128</td>
      <td>1.444743</td>
      <td>1.505235</td>
      <td>1.445150</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>16</td>
      <td>1.006285</td>
      <td>2.728409</td>
      <td>0.733065</td>
      <td>0.770287</td>
      <td>0.933039</td>
      <td>0.906151</td>
      <td>1.539201</td>
    </tr>
    <tr>
      <td>17</td>
      <td>1.789741</td>
      <td>3.931227</td>
      <td>2.959612</td>
      <td>2.891353</td>
      <td>3.181896</td>
      <td>3.096499</td>
      <td>4.060516</td>
    </tr>
    <tr>
      <td>18</td>
      <td>1.006839</td>
      <td>1.709025</td>
      <td>1.349434</td>
      <td>1.348100</td>
      <td>1.345061</td>
      <td>1.344675</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>19</td>
      <td>1.988646</td>
      <td>4.036760</td>
      <td>2.675012</td>
      <td>2.540842</td>
      <td>2.586160</td>
      <td>2.541795</td>
      <td>4.260632</td>
    </tr>
    <tr>
      <td>20</td>
      <td>2.135471</td>
      <td>2.019846</td>
      <td>2.166881</td>
      <td>2.122406</td>
      <td>2.189064</td>
      <td>2.160212</td>
      <td>2.868635</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.770301</td>
      <td>5.605109</td>
      <td>1.692946</td>
      <td>1.713511</td>
      <td>1.130376</td>
      <td>1.170718</td>
      <td>0.979217</td>
    </tr>
    <tr>
      <td>22</td>
      <td>1.122550</td>
      <td>3.811241</td>
      <td>1.458463</td>
      <td>1.447815</td>
      <td>0.760239</td>
      <td>0.766416</td>
      <td>0.692307</td>
    </tr>
    <tr>
      <td>23</td>
      <td>1.363178</td>
      <td>1.528132</td>
      <td>1.298698</td>
      <td>1.298169</td>
      <td>1.294669</td>
      <td>1.294283</td>
      <td>1.927272</td>
    </tr>
    <tr>
      <td>24</td>
      <td>1.668278</td>
      <td>3.804683</td>
      <td>3.806576</td>
      <td>3.521644</td>
      <td>3.674046</td>
      <td>3.519906</td>
      <td>6.242382</td>
    </tr>
    <tr>
      <td>25</td>
      <td>3.103629</td>
      <td>2.265445</td>
      <td>4.347855</td>
      <td>4.161862</td>
      <td>4.087820</td>
      <td>3.857038</td>
      <td>3.668334</td>
    </tr>
    <tr>
      <td>26</td>
      <td>1.863585</td>
      <td>3.964983</td>
      <td>4.207247</td>
      <td>4.218908</td>
      <td>3.586787</td>
      <td>3.616581</td>
      <td>2.462235</td>
    </tr>
    <tr>
      <td>27</td>
      <td>2.668776</td>
      <td>3.796216</td>
      <td>2.373441</td>
      <td>2.374979</td>
      <td>2.216759</td>
      <td>2.215797</td>
      <td>3.085220</td>
    </tr>
    <tr>
      <td>28</td>
      <td>1.367207</td>
      <td>5.320348</td>
      <td>0.825448</td>
      <td>0.817350</td>
      <td>0.832484</td>
      <td>0.826611</td>
      <td>0.834410</td>
    </tr>
    <tr>
      <td>29</td>
      <td>1.527729</td>
      <td>4.233459</td>
      <td>3.265057</td>
      <td>3.260141</td>
      <td>3.418992</td>
      <td>3.407364</td>
      <td>3.572366</td>
    </tr>
  </tbody>
</table>
</div>



**上面的表格代表着30列时间序列数据中，不同预测方法的MAPE。可以看出，无论是机器学习还是Space State模型。预测效果均很不错。 下面是30列时间序列数据中每列最优的预测模型**


```python
MAPE.idxmin(axis = 1)#对每一行求最值
```




    0        LSTM
    1          AR
    2     Thetakf
    3        Damp
    4     Thetakf
    5        LSTM
    6          RF
    7          RF
    8          RF
    9     Thetakf
    10         AR
    11       LSTM
    12         RF
    13         RF
    14      Theta
    15         RF
    16       ARkf
    17         RF
    18         RF
    19         RF
    20       LSTM
    21         RF
    22       Damp
    23      Theta
    24         RF
    25       LSTM
    26         RF
    27      Theta
    28         AR
    29         RF
    dtype: object

**在30列数据中，随机森林算法在13列种表现最优，限免我们看每种算法在30列上的平均MAPE值**


```python
MAPE.mean()
```

<img src="https://s1.ax1x.com/2020/08/15/dk63OU.png" alt="Unknown" border="0">


    RF         2.139925
    LSTM       3.884113
    ARkf       2.584056
    AR         2.531251
    Thetakf    2.452644
    Theta      2.443168
    Damp       3.017490
    dtype: float64



**在短期预测中，可以看出随机森林模型在最多的数据上表现最优，并且随机森林的平均MAPE最小。所以在这组数据（30列）中，随机森林模型在短期预测上表现最好**

### 长期预测（预测步长=30）
**除了改变步长外，其余操作均与短期预测相同**


```python
MAPE_RF_long,pred_RF_long = RF_pred(30)
```


```python
MAPE_LSTM_long,pred_LSTM_long=LSTM_pred(30)
```

    Epoch 1/1
     - 8s - loss: 0.0075
    Epoch 1/1
     - 8s - loss: 0.0238
    Epoch 1/1
     - 8s - loss: 0.0244
    Epoch 1/1
     - 8s - loss: 0.0220
    Epoch 1/1
     - 8s - loss: 0.0073
    Epoch 1/1
     - 8s - loss: 0.0144
    Epoch 1/1
     - 8s - loss: 0.0167
    Epoch 1/1
     - 8s - loss: 0.0213
    Epoch 1/1
     - 8s - loss: 0.0258
    Epoch 1/1
     - 8s - loss: 0.0067
    Epoch 1/1
     - 8s - loss: 0.0210
    Epoch 1/1
     - 8s - loss: 0.0165
    Epoch 1/1
     - 8s - loss: 0.0221
    Epoch 1/1
     - 8s - loss: 0.0255
    Epoch 1/1
     - 8s - loss: 0.0364
    Epoch 1/1
     - 8s - loss: 0.0270
    Epoch 1/1
     - 8s - loss: 0.0199
    Epoch 1/1
     - 8s - loss: 0.0185
    Epoch 1/1
     - 8s - loss: 0.0283
    Epoch 1/1
     - 8s - loss: 0.0229
    Epoch 1/1
     - 8s - loss: 0.0227
    Epoch 1/1
     - 8s - loss: 0.0314
    Epoch 1/1
     - 8s - loss: 0.0281
    Epoch 1/1
     - 8s - loss: 0.0224
    Epoch 1/1
     - 8s - loss: 0.0270
    Epoch 1/1
     - 8s - loss: 0.0215
    Epoch 1/1
     - 8s - loss: 0.0191
    Epoch 1/1
     - 8s - loss: 0.0153
    Epoch 1/1
     - 8s - loss: 0.0215
    Epoch 1/1
     - 8s - loss: 0.0241
    30列LSTM的MAPE值为



```python
plt.plot(df["AAPL.Close"])
plt.plot(range(271,301),pred_LSTM_long[0])
```

**以第一列数据（苹果公司）为例，这是LSTM长期预测结果**



<img src="https://s1.ax1x.com/2020/08/15/dk6YTJ.png" alt="output 47 1" border="0">



```python
MAPE_ARkf_long,MAPE_AR_long,MAPE_THkf_long,MAPE_TH_long,MAPE_Damp_long=Space_State(30)
```


```python
MAPE_long=pd.DataFrame()
MAPE_long["RF"]=MAPE_RF_long
MAPE_long["LSTM"]=MAPE_LSTM_long
MAPE_long["ARkf"]=MAPE_ARkf_long
MAPE_long["AR"]=MAPE_AR_long
MAPE_long["Thetakf"]=MAPE_THkf_long
MAPE_long["Theta"]=MAPE_TH_long
MAPE_long["Damp"]=MAPE_Damp_long
MAPE_long
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
**不同预测方法在30列数据上的预测MAPE为**

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RF</th>
      <th>LSTM</th>
      <th>ARkf</th>
      <th>AR</th>
      <th>Thetakf</th>
      <th>Theta</th>
      <th>Damp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>10.064228</td>
      <td>7.051741</td>
      <td>8.625701e+00</td>
      <td>8.598356</td>
      <td>7.936747</td>
      <td>7.959036</td>
      <td>8.835334</td>
    </tr>
    <tr>
      <td>1</td>
      <td>3.952841</td>
      <td>4.069136</td>
      <td>7.633074e+00</td>
      <td>7.242419</td>
      <td>4.878782</td>
      <td>4.740406</td>
      <td>6.621082</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.537938</td>
      <td>5.546962</td>
      <td>2.218464e+00</td>
      <td>2.170559</td>
      <td>3.652697</td>
      <td>3.641797</td>
      <td>2.302750</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.951732</td>
      <td>4.601494</td>
      <td>4.448392e+00</td>
      <td>4.428108</td>
      <td>3.261021</td>
      <td>3.270226</td>
      <td>3.296906</td>
    </tr>
    <tr>
      <td>4</td>
      <td>9.806659</td>
      <td>6.461248</td>
      <td>7.567996e+00</td>
      <td>7.526831</td>
      <td>7.153914</td>
      <td>7.178464</td>
      <td>6.903970</td>
    </tr>
    <tr>
      <td>5</td>
      <td>5.476482</td>
      <td>7.163879</td>
      <td>6.051534e+00</td>
      <td>7.081609</td>
      <td>5.898776</td>
      <td>6.040045</td>
      <td>6.283634</td>
    </tr>
    <tr>
      <td>6</td>
      <td>2.178420</td>
      <td>7.329152</td>
      <td>6.149928e+00</td>
      <td>6.081505</td>
      <td>4.833799</td>
      <td>4.809544</td>
      <td>5.101729</td>
    </tr>
    <tr>
      <td>7</td>
      <td>2.637032</td>
      <td>5.117404</td>
      <td>4.746260e+00</td>
      <td>4.737307</td>
      <td>4.490881</td>
      <td>4.491538</td>
      <td>4.816786</td>
    </tr>
    <tr>
      <td>8</td>
      <td>5.532063</td>
      <td>6.322065</td>
      <td>5.718674e+00</td>
      <td>5.539283</td>
      <td>2.436151</td>
      <td>2.394714</td>
      <td>2.823348</td>
    </tr>
    <tr>
      <td>9</td>
      <td>10.684089</td>
      <td>9.541481</td>
      <td>9.051459e+00</td>
      <td>8.851784</td>
      <td>8.814843</td>
      <td>8.851288</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>10</td>
      <td>6.154236</td>
      <td>5.623682</td>
      <td>5.049044e+00</td>
      <td>5.060927</td>
      <td>5.057464</td>
      <td>5.059505</td>
      <td>5.106153</td>
    </tr>
    <tr>
      <td>11</td>
      <td>16.064718</td>
      <td>9.866958</td>
      <td>1.809638e+01</td>
      <td>20.909875</td>
      <td>17.136112</td>
      <td>17.983040</td>
      <td>16.904855</td>
    </tr>
    <tr>
      <td>12</td>
      <td>4.368814</td>
      <td>5.999993</td>
      <td>1.816064e+60</td>
      <td>4.842531</td>
      <td>4.665608</td>
      <td>4.665980</td>
      <td>4.674504</td>
    </tr>
    <tr>
      <td>13</td>
      <td>5.833237</td>
      <td>5.726362</td>
      <td>7.341268e+00</td>
      <td>7.797259</td>
      <td>10.925084</td>
      <td>10.366154</td>
      <td>7.950700</td>
    </tr>
    <tr>
      <td>14</td>
      <td>5.052996</td>
      <td>3.692259</td>
      <td>2.585799e+00</td>
      <td>2.817539</td>
      <td>3.270044</td>
      <td>3.285099</td>
      <td>3.151113</td>
    </tr>
    <tr>
      <td>15</td>
      <td>2.648029</td>
      <td>4.120568</td>
      <td>3.218194e+00</td>
      <td>2.934791</td>
      <td>3.024734</td>
      <td>2.934794</td>
      <td>3.606390</td>
    </tr>
    <tr>
      <td>16</td>
      <td>2.008347</td>
      <td>5.026881</td>
      <td>7.001502e+00</td>
      <td>5.687707</td>
      <td>1.908207</td>
      <td>1.946526</td>
      <td>1.665622</td>
    </tr>
    <tr>
      <td>17</td>
      <td>1.830786</td>
      <td>2.883043</td>
      <td>3.675845e+00</td>
      <td>3.535430</td>
      <td>4.505273</td>
      <td>4.443322</td>
      <td>4.267481</td>
    </tr>
    <tr>
      <td>18</td>
      <td>2.195784</td>
      <td>3.534444</td>
      <td>3.225353e+00</td>
      <td>2.847462</td>
      <td>2.848219</td>
      <td>2.851733</td>
      <td>2.457517</td>
    </tr>
    <tr>
      <td>19</td>
      <td>6.322326</td>
      <td>5.265885</td>
      <td>5.474759e+00</td>
      <td>6.316218</td>
      <td>6.247093</td>
      <td>6.302859</td>
      <td>5.600986</td>
    </tr>
    <tr>
      <td>20</td>
      <td>2.110046</td>
      <td>3.439197</td>
      <td>4.639386e+00</td>
      <td>4.471560</td>
      <td>5.597658</td>
      <td>5.615262</td>
      <td>5.536040</td>
    </tr>
    <tr>
      <td>21</td>
      <td>1.415013</td>
      <td>2.078104</td>
      <td>2.000397e+00</td>
      <td>2.255690</td>
      <td>4.514409</td>
      <td>4.526155</td>
      <td>2.648775</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.735498</td>
      <td>3.065569</td>
      <td>2.292580e+00</td>
      <td>2.279342</td>
      <td>1.052101</td>
      <td>1.067912</td>
      <td>0.890288</td>
    </tr>
    <tr>
      <td>23</td>
      <td>2.909760</td>
      <td>2.828312</td>
      <td>2.846027e+00</td>
      <td>2.820178</td>
      <td>2.457782</td>
      <td>2.453874</td>
      <td>2.887378</td>
    </tr>
    <tr>
      <td>24</td>
      <td>5.170298</td>
      <td>8.576828</td>
      <td>8.352037e+00</td>
      <td>8.327080</td>
      <td>8.350483</td>
      <td>8.329110</td>
      <td>8.224116</td>
    </tr>
    <tr>
      <td>25</td>
      <td>4.056948</td>
      <td>5.413784</td>
      <td>5.058872e+00</td>
      <td>5.165013</td>
      <td>5.058881</td>
      <td>5.043171</td>
      <td>5.033189</td>
    </tr>
    <tr>
      <td>26</td>
      <td>1.164003</td>
      <td>2.484738</td>
      <td>2.226915e+00</td>
      <td>2.226555</td>
      <td>3.086225</td>
      <td>3.078351</td>
      <td>2.381321</td>
    </tr>
    <tr>
      <td>27</td>
      <td>2.164820</td>
      <td>4.688357</td>
      <td>7.283024e+00</td>
      <td>7.221789</td>
      <td>4.999716</td>
      <td>5.010607</td>
      <td>5.742219</td>
    </tr>
    <tr>
      <td>28</td>
      <td>1.685344</td>
      <td>1.883025</td>
      <td>1.720565e+00</td>
      <td>1.718161</td>
      <td>1.786672</td>
      <td>1.794010</td>
      <td>1.673491</td>
    </tr>
    <tr>
      <td>29</td>
      <td>2.177394</td>
      <td>2.554110</td>
      <td>2.859441e+00</td>
      <td>2.878399</td>
      <td>3.385886</td>
      <td>3.382222</td>
      <td>2.388501</td>
    </tr>
  </tbody>
</table>
</div>

**上面的表格代表着30列时间序列数据中，不同预测方法的MAPE。可以看出，无论是机器学习还是Space State模型。预测效果均很不错。 下面是30列时间序列数据中每列最优的预测模型**


```python
MAPE_long.idxmin(axis = 1)
```




    0        LSTM
    1          RF
    2          RF
    3          RF
    4        LSTM
    5          RF
    6          RF
    7          RF
    8       Theta
    9     Thetakf
    10       ARkf
    11       LSTM
    12         RF
    13       LSTM
    14       ARkf
    15         RF
    16       Damp
    17         RF
    18         RF
    19       LSTM
    20         RF
    21         RF
    22         RF
    23      Theta
    24         RF
    25         RF
    26         RF
    27         RF
    28       Damp
    29         RF
    dtype: object

**可以看出18列时间序列数据上预测表现最优**


```python
MAPE_long.mean()
```




    RF         4.329663
    LSTM       5.065222
    ARkf       6.053546
    AR         5.479040
    Thetakf    5.107842
    Theta      5.117225
    Damp       4.819868
    dtype: float64

<img src="https://s1.ax1x.com/2020/08/15/dk61yT.png" alt="Unknown 1" border="0">

**在长期预测中，可以看出随机森林模型在最多的数据上表现最优，并且随机森林的平均MAPE最小。所以在这组数据（30列）中，随机森林模型在短期预测上表现最好。但可以观察到长期预测相较短期预测效果稍差**

## 对比结果
使用MAPE作为预测评价标准。在30列数据中：

* **随机森林回归算法在短期预测中表现最优**
* **随机森林回归算法在长期预测这表现最优**
* **所有模型短期预测效果均好于长期预测**

